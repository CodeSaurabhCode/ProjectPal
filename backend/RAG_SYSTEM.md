# Advanced RAG System Documentation

## Overview

This implementation provides a production-ready RAG (Retrieval-Augmented Generation) system with:

- ‚úÖ **Semantic Search** - OpenAI embeddings (text-embedding-3-small, 1536 dimensions)
- ‚úÖ **Smart Chunking** - 4 strategies (sentence, paragraph, fixed, recursive)
- ‚úÖ **Vector Storage** - Local JSON + Azure Cosmos DB support
- ‚úÖ **Cosine Similarity** - Precise relevance scoring
- ‚úÖ **Citation Tracking** - Returns source passages with scores
- ‚úÖ **Caching** - In-memory embedding cache for performance

---

## Architecture

```
PM Handbook (docs/PM_handbook.txt)
        ‚îÇ
        ‚îú‚îÄ> ChunkingService
        ‚îÇ   ‚îî‚îÄ> Text chunks (400-600 chars, 100 char overlap)
        ‚îÇ
        ‚îú‚îÄ> EmbeddingService
        ‚îÇ   ‚îî‚îÄ> OpenAI API (text-embedding-3-small)
        ‚îÇ   ‚îî‚îÄ> Vector embeddings (1536 dimensions)
        ‚îÇ
        ‚îú‚îÄ> VectorStorageService
        ‚îÇ   ‚îî‚îÄ> Local: embeddings/pm-handbook.json
        ‚îÇ   ‚îî‚îÄ> Cosmos DB: (future)
        ‚îÇ
        ‚îî‚îÄ> RAG Tool (queryHandbookEnhanced)
            ‚îî‚îÄ> Similarity search (cosine distance)
            ‚îî‚îÄ> Top-K retrieval (default: 5)
            ‚îî‚îÄ> Citation formatting
```

---

## Setup & Initialization

### 1. Install Dependencies

Already installed in `package.json`:
- `@mastra/rag` - RAG utilities
- `@ai-sdk/openai` - OpenAI SDK

### 2. Initialize Embeddings

**First time setup:**

```bash
cd backend
npm run init-embeddings
```

This will:
1. Load PM handbook from `docs/PM_handbook.txt`
2. Chunk it into ~500 char pieces (recursive strategy)
3. Generate OpenAI embeddings for each chunk
4. Store in `embeddings/pm-handbook.json`
5. Run test search to verify

**Expected output:**
```
üöÄ Starting embedding initialization...

üìÅ Step 1: Initializing vector storage...
‚úÖ Storage initialized

üìñ Step 2: Loading PM handbook...
‚úÖ Loaded handbook from: D:\ProjectPal\backend\docs\PM_handbook.txt
   Size: 3245 characters

‚úÇÔ∏è  Step 3: Chunking document...
‚úÖ Created 12 chunks
   Strategy: Recursive (semantic-aware)
   Avg chunk size: 487 chars
   Min/Max: 234/598 chars
   Total words: 542

üß† Step 4: Generating embeddings...
   Using model: text-embedding-3-small
   Dimensions: 1536
   Batch processing: 12 chunks

‚úÖ Generated 12 embeddings
   Cache size: 12 entries

üíæ Step 5: Storing embeddings...
‚úÖ Stored embeddings
   Collection: pm-handbook
   Storage type: local
   Total documents: 12

üîç Step 6: Verifying with test search...
   Test query: "What are the budget approval requirements?"

‚úÖ Search test successful
   Found 3 relevant chunks:
   1. Score: 87.3% - For projects requiring budgets over $10,000, PMs must submit...
   2. Score: 72.5% - Budget approvals follow a tiered system based on amount...
   3. Score: 68.2% - All project kick-offs require formal documentation...

=================================================================
‚ú® EMBEDDING INITIALIZATION COMPLETE ‚ú®
=================================================================
Total time: 4.32s
Chunks created: 12
Embeddings generated: 12
Storage: local
Collection: pm-handbook

‚úÖ Your RAG system is ready to use!
```

### 3. Re-initialize When Needed

Run `npm run init-embeddings` again when:
- PM handbook content changes
- You want to adjust chunking parameters
- Vector index gets corrupted

---

## Usage

### In Chat Conversations

The enhanced RAG tool is automatically used when agents call `queryHandbook`:

```typescript
// Agent automatically uses this
const result = await agent.generate("What are the budget approval requirements?");

// Behind the scenes, it:
// 1. Generates embedding for the query
// 2. Finds similar chunks (cosine similarity)
// 3. Returns top 5 results with citations
```

### Example Chat Flow

**User:** "What's the budget approval process for a $15,000 project?"

**Agent (using enhanced RAG):**
```
Based on 3 relevant passages from the PM Handbook:

[Source 1, Relevance: 87%]:
For projects requiring budgets over $10,000, PMs must submit a budget 
approval form to the Finance team at least 5 business days before 
project kick-off. Approvals require signatures from both the department 
head and a finance controller.

[Source 2, Relevance: 72%]:
Budget approvals follow a tiered system: Under $5,000 - Team Lead approval, 
$5,000-$50,000 - Department Head + Finance, Over $50,000 - Executive 
Committee review.

[Source 3, Relevance: 68%]:
All budget submissions must include: project scope, timeline, resource 
allocation breakdown, and risk assessment. Incomplete submissions will 
be returned without review.
```

---

## Configuration

### Chunking Parameters

Edit `src/scripts/initialize-embeddings.ts`:

```typescript
const chunks = ChunkingService.chunk(handbookContent, COLLECTION_NAME, {
  strategy: 'recursive',    // 'sentence' | 'paragraph' | 'fixed' | 'recursive'
  maxChunkSize: 600,       // Max characters per chunk
  minChunkSize: 150,       // Min characters per chunk
  overlap: 100,            // Overlap between chunks (context)
  preserveSentences: true  // Try to break at sentence boundaries
});
```

**Recommendations:**
- **maxChunkSize: 400-600** - Optimal for embeddings
- **overlap: 50-150** - Helps maintain context across chunks
- **strategy: 'recursive'** - Best for mixed content (paragraphs + sentences)

### Search Parameters

Edit `src/mastra/tools/rag-tool-enhanced.ts`:

```typescript
const searchResults = await VectorStorageService.search(query, {
  topK: 5,           // Number of results to return
  threshold: 0.65    // Minimum similarity score (0-1)
});
```

**Recommendations:**
- **topK: 3-5** - Good balance between context and noise
- **threshold: 0.6-0.75** - Higher = more precise, lower = more recall

### Storage Type

Edit `.env`:

```bash
# Use local JSON storage (default)
VECTOR_STORAGE_TYPE=local

# Or use Cosmos DB (future)
VECTOR_STORAGE_TYPE=cosmos
```

---

## API Reference

### ChunkingService

```typescript
import { ChunkingService } from './services/ChunkingService';

// Chunk text
const chunks = ChunkingService.chunk(text, 'source-name', {
  strategy: 'recursive',
  maxChunkSize: 500,
  minChunkSize: 100,
  overlap: 50
});

// Get statistics
const stats = ChunkingService.getStats(chunks);
// {
//   totalChunks: 12,
//   avgChunkSize: 487,
//   minChunkSize: 234,
//   maxChunkSize: 598,
//   totalWords: 542,
//   totalChars: 5844
// }
```

### EmbeddingService

```typescript
import { EmbeddingService } from './services/EmbeddingService';

// Generate single embedding
const embedding = await EmbeddingService.generateEmbedding('text to embed');
// {
//   id: 'emb-1234-text-to-embed',
//   text: 'text to embed',
//   vector: [0.123, -0.456, ...], // 1536 dimensions
//   model: 'text-embedding-3-small',
//   dimensions: 1536
// }

// Generate batch
const embeddings = await EmbeddingService.generateEmbeddingsBatch([
  'text 1',
  'text 2',
  'text 3'
]);

// Calculate similarity
const similarity = EmbeddingService.cosineSimilarity(vec1, vec2);
// 0.87 (87% similar)

// Find most similar
const similar = EmbeddingService.findSimilar(queryEmb, candidateEmbs, 5, 0.7);
// [
//   { id: '...', text: '...', score: 0.89 },
//   { id: '...', text: '...', score: 0.76 }
// ]
```

### VectorStorageService

```typescript
import { VectorStorageService } from './services/VectorStorageService';

// Initialize
await VectorStorageService.initialize();

// Store embeddings
await VectorStorageService.storeEmbeddings(embeddings, 'collection-name');

// Search
const results = await VectorStorageService.search('query text', {
  topK: 5,
  threshold: 0.7,
  source: 'pm-handbook'
});

// Get stats
const stats = await VectorStorageService.getStats();
// {
//   totalDocuments: 12,
//   totalEmbeddings: 12,
//   avgVectorDimensions: 1536,
//   storageType: 'local',
//   collection: 'pm-handbook'
// }

// Delete collection
await VectorStorageService.deleteCollection('collection-name');
```

---

## Performance

### Benchmarks (PM Handbook, ~3KB)

| Operation | Time | Notes |
|-----------|------|-------|
| **Initial Setup** | 3-5s | One-time cost |
| Chunking | <50ms | 12 chunks created |
| Embedding generation | 3-4s | 12 chunks @ ~300ms each |
| Storage (local) | <10ms | JSON write |
| Search query | 150-300ms | Includes embedding + search |

### Optimization Tips

1. **Cache embeddings** - Already implemented, saves 90% time on repeated queries
2. **Batch processing** - Max 100 texts per batch (rate limit protection)
3. **Local storage** - Fast for <1000 chunks, use Cosmos DB for larger datasets
4. **Increase threshold** - Higher threshold = fewer results = faster search

---

## Troubleshooting

### Error: "OpenAI API error: 401"

**Cause:** Invalid or missing `OPENAI_API_KEY`

**Fix:**
```bash
# Check .env file
OPENAI_API_KEY=sk-proj-...

# Verify key is loaded
echo $env:OPENAI_API_KEY  # Windows
echo $OPENAI_API_KEY      # Linux/Mac
```

### Error: "Handbook file not found"

**Cause:** `docs/PM_handbook.txt` missing

**Fix:**
```bash
# Verify file exists
ls backend/docs/PM_handbook.txt

# If missing, create it
mkdir -p backend/docs
cp your-handbook.txt backend/docs/PM_handbook.txt
```

### Error: "No documents found in collection"

**Cause:** Embeddings not initialized

**Fix:**
```bash
cd backend
npm run init-embeddings
```

### Search Returns No Results

**Possible causes:**
1. **Threshold too high** - Lower from 0.7 to 0.5
2. **Query too vague** - Be more specific
3. **Embeddings not initialized** - Run `npm run init-embeddings`

---

## Future Enhancements

### Phase 1 (Next Week)
- [ ] Add hybrid search (vector + keyword)
- [ ] Implement re-ranking algorithm
- [ ] Add metadata filtering
- [ ] Cache search results

### Phase 2 (Next Month)
- [ ] Cosmos DB vector storage
- [ ] Multi-document support
- [ ] Automatic re-indexing on file changes
- [ ] Query expansion (synonyms, related terms)

### Phase 3 (Future)
- [ ] Fine-tuned embeddings
- [ ] Cross-encoder re-ranking
- [ ] Document versioning
- [ ] A/B testing framework

---

## Cost Analysis

### OpenAI Embeddings Pricing

- **Model:** text-embedding-3-small
- **Price:** $0.00002 / 1K tokens (~$0.02 / 1M tokens)
- **PM Handbook:** ~12 chunks √ó 150 tokens = 1,800 tokens
- **Cost:** ~$0.00004 (basically free)

### Storage Costs

- **Local:** Free (JSON files)
- **Cosmos DB:** ~$25/month (1 RU/s)

---

## Testing

```bash
# Test chunking
tsx src/services/ChunkingService.test.ts

# Test embeddings
tsx src/services/EmbeddingService.test.ts

# Test vector storage
tsx src/services/VectorStorageService.test.ts

# Test RAG tool
tsx src/mastra/tools/rag-tool-enhanced.test.ts
```

---

## Support

For issues or questions:
1. Check this documentation
2. Review error logs in console
3. Verify `.env` configuration
4. Re-run `npm run init-embeddings`

---

**Last Updated:** October 25, 2025
**Version:** 1.0.0
